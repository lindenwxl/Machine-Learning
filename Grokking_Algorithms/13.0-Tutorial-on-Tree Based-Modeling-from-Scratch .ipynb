{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "基于树的学习算法被认为是最优秀学习方法之一，它主要用于监督学习。树方法帮助预测模型拥有较高的精度，稳定性、易于解释。与线性模型不同，树模型映射非线性关系相当好。适用于解决手头上任何类型的问题（``分类或回归``）。\n",
    "\n",
    "决策树、随机森林、gradient boosting等方法被广泛用于各种数据学科问题中。因此，对于每个分析师（包括新人）来说，学习这些算法并用它们建模很重要。\n",
    "\n",
    "本教程旨在帮助初学者学习树建模。成功学完本教程后，有望成为一个精通使用树算法，建立预测模型的人.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\n",
    "\n",
    "Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), it’s important to learn these algorithms and use them for modeling.\n",
    "\n",
    "This tutorial is meant to help beginners learn tree based modeling from scratch. After the successful completion of this tutorial, one is expected to become proficient at using tree based algorithms and build predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- ①、什么是决策树？它是如何工作的？\n",
    "- ②、回归树和分类树\n",
    "- ③、一棵树是如何决定在哪里分裂？\n",
    "- ④、树建模的关键参数是什么？我们如何避免决策树的过拟合？\n",
    "- ⑤、基于树的模型优于线性模型吗？\n",
    "- ⑥、Python中使用决策树\n",
    "- ⑦、树建模的集成方法是什么？\n",
    "- ⑧、什么是Bagging？它是如何工作的？\n",
    "- ⑨、什么是随机森林？它是如何工作的？\n",
    "- ⑩、什么是Boosting？它是如何工作的？\n",
    "- 11、GBM和Xgboost哪个更强大？\n",
    "- 12、Python中使用GBM\n",
    "- 13、Python中使用XGBoost\n",
    "- 14、在哪里实践？\n",
    "\n",
    "## Table of Contents\n",
    "- ①、 What is a Decision Tree? How does it work?\n",
    "- ②、Regression Trees vs Classification Trees\n",
    "- ③、How does a tree decide where to split?\n",
    "- ④、What are the key parameters of model building and how can we avoid over-fitting in decision trees?\n",
    "- ⑤、Are tree based models better than linear models?\n",
    "- ⑥、Working with Decision Trees in R and Python\n",
    "- ⑦、What are the ensemble methods of trees based model?\n",
    "- ⑧、What is Bagging? How does it work?\n",
    "- ⑨、What is Random Forest ? How does it work?\n",
    "- ⑩、What is Boosting ? How does it work?\n",
    "- 11、Which is more powerful: GBM or Xgboost?\n",
    "- 12、Working with GBM in R and Python\n",
    "- 13、Working with Xgboost in R and Python\n",
    "- 14、Where to Practice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1、什么是决策树？它是如何工作的？\n",
    "\n",
    "<img align=\"rigth\" src=\"figures/tree1.png\">\n",
    "\n",
    "决策树是一种监督学习算法（有一个预定义的目标变量）主要用于分类问题。它适用于离散的和连续的输入和输出变量。该技术中，将人口或样本分成两个或两个以上同质组（或者子集），以输入变量中最重要的差异A/不同为依据进行分割。\n",
    "\n",
    "**例如：**\n",
    "假设我们有一个样本，30名学生包含三个变量：性别（男/女），班级（IX/X）和身高（5到6英尺）。这30名学生中有15个名空闲时打板球。现在，我想建立一个模型来预测谁会在空闲时打板球？在这个问题上，我们需要根据非常重要的三个输入变量来分开空闲时打板球的学生。\n",
    "\n",
    "这正是决策树可以帮助的，它将依据三个变量并识别创造最同质的学生组（互为异构）的变量，来分开学生。在下面的图片中，可以看到与其它两个变量相比，性别能够识别最佳同质组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Decision Tree ? How does it work ?\n",
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "**Example:-**\n",
    "Let’s say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n",
    "\n",
    "This is where decision tree helps, it will segregate the students based on all values of three variable and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.\n",
    "\n",
    "<img align=\"rigth\" src=\"figures/tree2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所述，决策树识别了最重要的变量并给出最佳的同质组人口值。现在问题是，它是如何识别并区分变量？要做到这一点，决策树使用各种算法，这些将在下面的章节讨论。\n",
    "\n",
    "As mentioned above, decision tree identifies the most significant variable and it’s value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To do this, decision tree uses various algorithms, which we will shall discuss in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的类型\n",
    "\n",
    "决策树的类型基于目标变量的类型。它可以有两种类型：\n",
    "-    1.``离散变量决策树``：有明确目标变量的决策树被称为离散变量决策树。**例如**：-在上面学生问题的情景中，目标变量是“学生将打板球与否”即是或否。\n",
    "\n",
    "\n",
    "-    2.``连续变量决策树``：有连续目标变量的决策树被称为连续变量决策树。\n",
    "**例如**：假设有一个问题，预测客户是否会支付保险公司的续保费用（是/否）。在这里，客户的收入是一个重要的变量，但保险公司并没有所有客户的收入明细。现在，我们知道这是一个重要的变量，可以根据职业、作品和其它各种变量建立一个决策树来预测客户收入。在这种情况下，预测的是连续变量值。\n",
    "\n",
    "## Types of Decision Trees\n",
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "- Categorical Variable Decision Tree: Decision Tree which has categorical target variable then it called as categorical variable decision tree. \n",
    "\n",
    "**Example**:- In above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.\n",
    "\n",
    "\n",
    "- Continuous Variable Decision Tree: Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n",
    "\n",
    "**Example**:- Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/ no). Here we know that income of customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与决策树相关的重要术语\n",
    "使用决策树时的基本术语：\n",
    "-    1.根节点：它代表整个人口或样本，并进一步分为两个或两个以上的同质组。\n",
    "-    2.分割：它是一个将一个节点划分成两个或两个以上子节点的过程。\n",
    "-    3.决策节点：当一个子节点进一步分裂成子节点，那么它被称为决策节点。\n",
    "-    4.叶/终端节点：不分裂的节点被称为叶子或终端节点。\n",
    "-    5.剪枝：删除决策节点的子节点的过程被称为剪枝。你可以说相反的过程叫分裂。\n",
    "-    6.分支 /子树：整个树的子部分被称为分支或子树。\n",
    "-    7.父子节点：一个节点，分为子节点被称为子节点的父节点，而子节点是父节点的孩子。\n",
    "\n",
    "这些都是常用的决策树术语。我们知道，每种算法都有优缺点，下面是应该知道的重要因素。\n",
    "\n",
    "## Important Terminology related to Decision Trees\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "-    1.Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "-    2.Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "-    3.Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "-    4.Leaf/ Terminal Node: Nodes do not split is called Leaf or Terminal node.\n",
    "-    5.Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "-    6.Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n",
    "-    7.Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.\n",
    "\n",
    "<img align=\"rigth\" src=\"figures/tree3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点\n",
    "-    1.易于理解：决策树的输出非常容易理解，即使是非分析背景的人。不需要任何统计知识来阅读和解释它们。它的图形表示非常直观，用户可以很容易地关联他们的假设。\n",
    "-    2.有用的数据探索：决策树是最快的识别最重要变量和两个或两个以上变量之间的关系的方式之一。在决策树的帮助下，我们可以创建新的具有更强能力的变量/特征来预测目标变量。你可以参考文章（提高回归模型力量的技巧）一条技巧。它也可以用于数据探索方面。例如，我们正在研究一个问题，我们有数百个变量的信息，此时决策树将有助于识别最重要的变量。\n",
    "-    3.较少的数据清理要求：与其它建模技术相比，它需要更少的数据清理。它的公平度不受异常值和缺失值的影响。\n",
    "-    4.数据类型不受限制：它可以处理连续数值和离散变量。\n",
    "-    5.非参数方法：决策树被认为是一种非参数方法。这意味着决策树没有空间分布和分类器结构。\n",
    "\n",
    "### 缺点\n",
    "-    1.过拟合：过拟合是决策树模型最实际的困难之一。这个问题通过限定参数模型和修剪得到解决（详细讨论如下）。\n",
    "-    2.不适合连续变量：在处理连续数值变量时，决策树用不同类别分类变量丢失信息。\n",
    "\n",
    "### Advantages\n",
    "-    1.Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "-    2.Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables / features that has better power to predict target variable. You can refer article (Trick to enhance power of regression model) for one such trick.  It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "-    3.Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "-    4.Data type is not a constraint: It can handle both numerical and categorical variables.\n",
    "-    5.Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "\n",
    "### Disadvantages\n",
    "-    1.Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "-    2.Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、回归树和分类树\n",
    "\n",
    "<img align=\"rigth\" src=\"figures/tree4.png\">\n",
    "\n",
    "我们都知道，终端节点（或叶子）位于决策树的底部。这意味着，决策树绘制时通常是颠倒的，叶子在底部，根在顶部（如上所示）。\n",
    "\n",
    "这两种树的工作几乎相似，我们看看分类树和回归树的主要差异和相似性：\n",
    "-    1.回归树用于因变量是连续时。分类树用于因变量是离散时。\n",
    "-    2.对于回归树，通过训练数据的终端节点获取的值是观察到的落在该区域的平均响应。因此，如果一个未知结果数据落在该区域，我们将用平均值进行预测。\n",
    "-    3.对于分类树，通过训练数据的终端节点获取的值（分类）是观察落在该区域的模式。因此，如果一个未知分类的数据落在该区域，我们将用模式值进行预测。\n",
    "-    4.两种树划分预测空间（自变量）为不同且无重叠的区域。为了简单起见，你可以把这些区域想象为高维盒子或箱子。\n",
    "-    5.两种树遵循自上而下的贪婪方法被称为递归二分分裂。我们叫它“自上而下”，因为当所有的观察都在一个单一的区域时它从树的顶部开始，并依次将预测空间分裂成两个新的分支。它被称作“贪婪”是因为，该算法只关心（寻找最佳可用变量）当前分裂，而不关心未来会带来更好树的分裂。\n",
    "-    6.这个分裂过程一直持续直到达到一个用户定义的停止标准。例如：一旦每个节点的观测值不到50，我们可以告诉该算法停止。\n",
    "-    7.在这两种情况下，分裂过程直到达到停止标准导致树完全生长。但是，完全生长的树可能过拟合数据，导致看不见数据精度差。这带来了‘修剪’。修剪是用来解决过拟合技术的一种。关于它我们将了解更多在以下部分。\n",
    "\n",
    "\n",
    "##  Regression Trees vs Classification Trees\n",
    "We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leaves are the the bottom & roots are the tops (shown below).\n",
    "\n",
    "Both the trees work almost similar to each other, let’s look at the primary differences & similarity between classification and regression trees:\n",
    "\n",
    "- Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical.\n",
    "- In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value.\n",
    "- In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value.\n",
    "- Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes.\n",
    "- Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.\n",
    "- This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the the algorithm to stop once the number of observations per node becomes less than 50.\n",
    "- In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting. We’ll learn more about it in following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、一棵树如何决定在哪里分裂？\n",
    "\n",
    "制定分裂策略的决策严重影响一棵树的准确性。分类树和回归树的决策标准是不同的。\n",
    "\n",
    "\n",
    "决策树使用多种算法来决定将一个节点分裂成两个或两个以上子节点。子节点的创建增加了子节点的同质性。换句话说，相对于目标变量节点纯度的增加。决策树分裂所有可用变量的节点，然后选择产生最多同质性节点的分裂。\n",
    "\n",
    "\n",
    "该算法选择也基于目标变量的类型。让我们来看看决策树中最常用的四种算法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼系数\n",
    "\n",
    "基尼系数表示，如果我们从样本随机选择两个项，它们一定是同一类的，那么样本的纯度可能是1。\n",
    "-    1.它对分类目标变量“成功”或“失败”可用。\n",
    "-    2.它只执行二元分裂。\n",
    "-    3.基尼系数越高，同质性越高。\n",
    "-    4.CART（分类树和回归树）使用基尼方法来创建二元分裂。\n",
    "\n",
    "**计算分裂的基尼系数的步骤**\n",
    "\n",
    "    1.计算子节点的基尼系数，使用公式成功概率平方和失败概率平方的总和（p^2+q^2）。\n",
    "    2.计算分裂的基尼系数，使用该分割的每个节点的加权基尼得分。\n",
    "    \n",
    "**举例**：—参照上面的例子，我们想基于目标变量（打板球与否）分开学生。在下面的图片，我们使用性别和班级两个输入变量分开人口。现在，*我想使用基尼系数确定哪个分割产生的同质子节点更多*。\n",
    "<img align=\"rigth\" src=\"figures/tree5.png\">\n",
    "\n",
    "**按性别分**：\n",
    "-    1.计算，女性子节点的基尼系数 = (0.2)*(0.2)+(0.8)*(0.8)=0.68\n",
    "-    2.男性子节点的基尼系数 = (0.65)*(0.65)+(0.35)*(0.35)=0.55\n",
    "-    3.计算按性别分的加权基尼系数  = (10/30)*0.68+(20/30)*0.55=0.59\n",
    "    \n",
    "    \n",
    "**按班级分**：\n",
    "-    1.IX班级子节点的基尼系数  = (0.43)*(0.43)+(0.57)*(0.57)=0.51\n",
    "-    2.X班级子节点的基尼系数  =  (0.56)*(0.56)+(0.44)*(0.44)=0.51\n",
    "-    3.计算按班级分的加权基尼系数 = (14/30)*0.51+(16/30)*0.51=0.51\n",
    "\n",
    "\n",
    "上述，你可以看到按``性别分裂``的基尼分数高于按``班级分裂``，因此，该节点分裂将发生在性别上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卡方\n",
    "\n",
    "它是一个找出父子节点差异的统计显著性的算法。我们通过目标变量的观察和期望频率之间的标准差异的平方和来测量它。\n",
    "-    1.它对分类目标变量“成功”或“失败”可用。\n",
    "-    2.它可以执行两个或两个以上分裂。\n",
    "-    3.卡方值越高，父子节点差异的统计显著性越高。\n",
    "-    4.每个节点的卡方计算使用公式，卡方 = ((实际– 期望)^2 / 期望)^1/2\n",
    "-    5.它生成的树被称为CHAID （卡方自动交互检测）\n",
    "\n",
    "\n",
    "**计算分裂的卡方的步骤**：\n",
    "-    1.计算单个节点的卡方，通过计算成功与失败的偏差\n",
    "-    2.计算分裂的卡方，使用该分割的所有节点的成功与失败的卡方的总和\n",
    "\n",
    "**举例**：使用上面计算基尼系数的例子。\n",
    "\n",
    "**按性别分**：\n",
    "\n",
    "-    1.首先填充女性节点，填充“打板球”和“不打板球”的实际值，这里分别是2和8。\n",
    "-    2.计算“打板球”和“不打板球”的期望值，这里将两个都是5，因为父节点有50%的概率，我们运用同样的概率到女性数上（10）。\n",
    "-    3.使用公式计算偏差，实际 - 期望。“打板球”（2 - 5 = -3），“不打板球”（8 - 5 = 3）。\n",
    "-    4.使用公式=((实际– 期望)^2 / 期望)^1/2计算“打板球”和“不打板球”节点的卡方。您可以参考下表计算。\n",
    "-    5.按照相似的步骤计算男性节点的卡方值。\n",
    "-    6.现在加上所有的卡方值来计算按性别分裂的卡方。\n",
    "\n",
    "\n",
    "**综述，你可以看到卡方也确定按性别比按班级分裂更显著。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**按性别分：**\n",
    "\n",
    "<img align=\"left\" src=\"figures/tree6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**按班级分**：\n",
    "按班级分裂执行类似的计算步骤，你会得出下面的表。\n",
    "\n",
    "<img align=\"left\" src=\"figures/tree7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益：\n",
    "\n",
    "看看下面的图片，认为哪个节点很容易被描述。我敢肯定，你的答案是C，由于所有的值都是相似的，它需要更少的信息。另一方面，B需要更多的信息来描述它，A需要最多的信息。换句话，我们可以说，C是一个纯净的节点，B较少纯，A更不纯。\n",
    "\n",
    "<img align=\"left\" src=\"figures/tree8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以得出一个结论，较少纯净的节点需要较少的信息来描述它。更不纯的节点需要较多信息。信息论定义系统中这种混乱程度称为熵是一个措施。如果样本完全同质，那么``熵``是零，如果样本同样划分（50%—50%），熵是一。\n",
    "\n",
    "熵可以用公式计算： $熵 = -p*log_2{p} - q*log_2{q}$\n",
    "\n",
    "这里p和q分别是该节点成功和失败的概率。熵也用于分类目标变量。它选择与父节点和其它分裂相比，具有最低熵的分裂。熵越小，越好。\n",
    "\n",
    "**计算分裂的熵的步骤**：\n",
    "-    1.计算父节点的熵\n",
    "-    2.计算分裂的每个节点的熵并计算分裂中所有可用子节点的加权平均值。\n",
    "\n",
    "**举例**：学生的例子让我们使用这个方法来确定最佳的分割。\n",
    "-    1.父节点的熵= -(15/30) log2 (15/30) – (15/30) log2 (15/30) = 1。此处1显示它不是一个纯净的节点。\n",
    "-    2.女性节点的熵 = -(2/10) log2 (2/10) – (8/10) log2 (8/10) = 0.72，男性节点的熵 -(13/20) log2 (13/20) – (7/20) log2 (7/20) = 0.93\n",
    "-    3.按性别分裂的熵 = 子节点的加权熵 = (10/30)*0.72 + (20/30)*0.93 = **0.86**\n",
    "\n",
    "\n",
    "-    4.班级IX节点的熵，-(6/14) log2 (6/14) – (8/14) log2 (8/14) = 0.99，班级X节点的熵，-(9/16) log2 (9/16) – (7/16) log2 (7/16) = 0.99。\n",
    "-    5.按班级分裂的熵 =  (14/30)*0.99 + (16/30)*0.99 = **0.99**\n",
    "\n",
    "\n",
    "上述，你可以看到，按性别分裂熵最低，所以该树将会按性别分裂。我们可以从熵中得到信息增益为**`1-熵`**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 减少方差\n",
    "\n",
    "至今，我们讨论了分类目标变量的算法。减少方差算法用于**连续目标变量（回归问题）**。该算法采用方差的标准公式来选择最佳分裂。较低方差的分裂被选来作为人口分裂的标准：\n",
    "\n",
    "$方差 =\\frac{\\sum(X -\\overline{X})}{N}\\qquad $\n",
    "\n",
    "上述X拔是平均值，X是实际值，n是值的个数。\n",
    "**计算方差的步骤**：\n",
    "    1.计算每个节点的方差。\n",
    "    2.计算每个分裂的方差作为每个节点方差的加权平均值。\n",
    "    \n",
    "    \n",
    "**举例**：我们分配打板球数值1，不打板球数值0。现在按照步骤来确定正确的分裂：\n",
    "-  1.根节点方差，平均值是(15*1 + 15*0)/30 = 0.5，有15个1和15个0。方差是((1-0.5)^2+(1-0.5)^2+….15 次+(0-0.5)^2+(0-0.5)^2+…15 次) / 30，这可以写成 (15*(1-0.5)^2+15*(0-0.5)^2) / 30 = **0.25**\n",
    "\n",
    "-    2.女性节点平均值 =  (2*1+8*0)/10=0.2，方差 = (2*(1-0.2)^2+8*(0-0.2)^2) / 10 = 0.16\n",
    "-    3.男性节点平均值 = (13*1+7*0)/20=0.65 ，方差 = (13*(1-0.65)^2+7*(0-0.65)^2) / 20 = 0.23\n",
    "-    4.按性别分裂的方差 = 子节点的加权方差 = (10/30)*0.16 + (20/30) *0.23 = **0.21**\n",
    "-    5.班级IX节点的平均值=  (6*1+8*0)/14=0.43，方差 = (6*(1-0.43)^2+8*(0-0.43)^2) / 14= 0.24\n",
    "-    6.班级X节点的平均值 =  (9*1+7*0)/16=0.56，方差 = (9*(1-0.56)^2+7*(0-0.56)^2) / 16 = 0.25\n",
    "-    7.按班级分裂的方差 = (14/30)*0.24 + (16/30) *0.25 = **0.25**\n",
    "\n",
    "上述，你可以看到，按性别分裂的方差低于父节点，因此分裂将于性别变量。\n",
    "*直到这里，我们了解了决策树的基本知识和决策过程为选择最佳分裂建立树模型相关的。正如我所说，决策树可以应用于回归和分类问题。让我们详细地了解这些方面。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、树建模的关键参数是什么？我们如何避免决策树的过拟合？\n",
    "\n",
    "过拟合是决策树建模时面临的关键挑战之一。如果是没有限制组的决策树，它会给你100%准确训练集，因为最坏的情况下，每个观察最终以1个叶子结束。因此，**决策树建模时阻止过拟合是关键**，它可以通过2种方式：\n",
    " -   1.限制树的大小\n",
    " -   2.剪枝\n",
    "让我们简要地讨论一下这两种方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"figures/tree9.png\">\n",
    "\n",
    "### 限制树的大小\n",
    "这可以通过定义树使用的各种参数来完成。首先，一棵决策树的基本结构：\n",
    "\n",
    "用于定义树的参数进一步说明如下。下面描述的参数与工具无关。重要的是理解树建模中使用的参数的作用。这些参数在Python是可用的。\n",
    "\n",
    "#### 1.节点分裂的最小样本\n",
    "\n",
    "- 定义要分裂节点所需的最少样本数（或观测值）。\n",
    "\n",
    "- 用来控制过拟合。更高的值，防止模型学习关系具体到特定的样本。\n",
    "\n",
    "- 过高的值可能引起欠拟合，使用CV值调节。\n",
    "\n",
    "#### 2.终端节点（叶）的最少样本数\n",
    "\n",
    "- 定义了树终端节点或叶子所需要的最少的样本数\n",
    "\n",
    "- 类似min_samples_split用来控制过拟合。\n",
    "\n",
    "- 针对不平衡分类问题通常选择较低的值，因为大部分少数类的区域将非常小。\n",
    "\n",
    "#### 3.树的最大深度（垂直深度） \n",
    "\n",
    "- 树的最大深度。\n",
    "\n",
    "- 用来控制过拟合，更高的深度将允许模型学习关系非常具体到特定的样本。\n",
    "\n",
    "- 使用CV值调节。\n",
    "\n",
    "#### 4.终端节点的最大数量\n",
    "\n",
    "- 树的终端节点或叶子的最多个数。\n",
    "\n",
    "- 可能在max_depth中定义。深度为'n'的二叉树被构造就会产生最多2^n个终端节点。\n",
    "\n",
    "#### 5.考虑分割的最大特征数\n",
    "\n",
    "- 为了寻找最佳分裂的特征数。这些将被随机选择。\n",
    "\n",
    "- 根据经验，总特征数的平方根就行，但是应该检查总特征数的30-40%。\n",
    "\n",
    "- 过高的值可能会导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"figures/tree10.png\">\n",
    "\n",
    "## 剪枝\n",
    "\n",
    "如前面所讨论的，设置限制的技术是一个贪婪的方法。换句话说，它将检查即刻的最佳分裂，向前移动，直到到达指定的停止条件之一。当你开车时，让我们考虑以下情况：\n",
    "\n",
    "\n",
    "有2个车道：\n",
    "-    1.一条汽车以80公里/小时行驶\n",
    "-    2.一条卡车以30公里/小时行驶\n",
    "\n",
    "\n",
    "这时，你是黄色的车，有2个选择：\n",
    "-    1.左转，迅速超过其它2辆汽车\n",
    "-   2.继续在本车道行驶\n",
    "\n",
    "我们分析这两种选择。前一个选择，你将立即超越前面的车，并到达卡车的后面，开始以30公里/小时行驶，寻找机会回到右边。同时所有起初在你后面的车领先。这将是最佳选择，如果你的目标是在接下来的10s里行进最远的距离。后一个选择，你以相同的速度，跨过卡车，然后超车可能视情况而定。贪婪的你！\n",
    "\n",
    "<img align=\"right\" src=\"figures/tree11.png\">\n",
    "这正是通常的决策树和剪枝之前的区别。有限制的决策树不会看前面的卡车，通过左转采取贪婪的方法。另一方面，如果我们使用剪枝，我们实际上向前看几步，并做出选择。\n",
    "\n",
    "\n",
    "所以我们知道剪枝更好。但是如何在决策树中实现它？这个想法很简单。    \n",
    "\n",
    "-    1.首先，我们使决策树到一个很大的深度。    \n",
    "\n",
    "-    2.然后，我们开始在底部删除与顶部相比给我们负回报的叶子。    \n",
    "\n",
    "-    3.假设一个分裂给我们增加-10（损失10），然后下一个分裂我们获得了10。一个简单的决策树将停止在步骤1剪枝，我们会看到整体收益是10，并保留两个叶子。\n",
    "\n",
    "**注意，sklearn的决策树分类器目前不支持剪枝。像xgboost这样先进的包在它们的实现里已经采用剪枝**。R中的rpart库提供了一个函数来剪枝。对R用户是好消息！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、基于树的模型优于线性模型吗？\n",
    "\n",
    "我们中的很多人都有这个疑问“如果分类问题我可以使用逻辑回归，回归问题可以使用线性回归，为什么需要使用树？”而且，这也是一个有效的一个。\n",
    "\n",
    "事实上，你可以使用任何算法。它取决于你正在解决的问题的类型。让我们看一些关键因素，这将有助于你决定使用哪种算法：    \n",
    "\n",
    " -   1.如果因变量和自变量之间的关系是近似一个线性模型，线性回归将优于树模型。    \n",
    "\n",
    " -   2.如果因变量与自变量之间的是一个高非线性并复杂的关系，树模型将优于经典的回归方法。    \n",
    "\n",
    " -   3.如果你需要建立一个易于向人们解释的模型，决策树模型将永远比线性模型做得好。决策树模型比线性回归解释起来更简单！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、Python中使用决策树\n",
    "\n",
    "对于Python用户，决策树很容易实现。让我们快速看一组代码，它可以让你开始使用这个算法。为了便于使用，我分享标准代码，你需要更换你的数据集名和变量来开始。\n",
    "\n",
    "```python\n",
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、基于树的建模的集成方法是什么？\n",
    "\n",
    "<img align=\"left\" src=\"figures/tree12.png\">\n",
    "\n",
    "词“集合”的字面意思是``群``。集成方法涉及到预测模型，以实现精度更高和模型稳定性。**集成方法以授予最高提升树的基本模型而著称**。\n",
    "像其他模型，树模型也存在偏差和方差的烦扰。偏差的意思是，‘预测值的平均值不同于实际值是多少。’方差的意思是，‘相同人口的不同样本在相同点上模型预测的差异’。\n",
    "\n",
    "\n",
    "你建立一个小树，你会得到一个低方差和高偏差的模型。你如何权衡偏差和方差？\n",
    "\n",
    "\n",
    "通常，**``当增加模型的复杂性，由于模型中较低的偏差，你将看到预测误差减少。当继续让模型更复杂，模型最终会过拟合而且开始遭遇高方差``**。\n",
    "\n",
    "\n",
    "一流的模型应该保持这两种类型错误的平衡。这就是所谓的**偏差方差权衡管理**。集成学习是实现这种权衡分析的一种方法。\n",
    "\n",
    "一些常用的集成方法包括：Bagging，Boosting和Stacking。在本教程中，我们将主要探讨Bagging和Boosting的细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、什么是Bagging？它是如何工作的？\n",
    "\n",
    "Bagging是一种技术，通过结合相同数据集的不同子样本的多分类器模型的结果，以减少我们预测的方差。下图将使它更清晰：\n",
    "\n",
    "<img align=\"right\" src=\"figures/tree13.png\">\n",
    "\n",
    "bagging的步骤是：\n",
    "#### 1.创建多个数据集：\n",
    "\n",
    "- 采样在原数据有放回和形成新数据完成。\n",
    "\n",
    "- 新数据集可以有列和行的一部分，这通常是bagging模型中的超参数\n",
    "\n",
    "- 取行和列部分小于1有助于模型健壮，不易于过拟合\n",
    "\n",
    "#### 2.建立多个分类器：\n",
    "\n",
    "- 分类器建立在每个数据集。\n",
    "\n",
    "- 通常相同的分类器在每个数据集建模并预测。\n",
    "\n",
    "#### 3.组合分类器：\n",
    "\n",
    "- 所有分类器的预测进行组合，根据手头的问题通过平均值，中值或模值。\n",
    "\n",
    "- 组合值一般比单一模型更健壮。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里建造的模型的数量不是超参数。模型数量较多比较少总是更好或可能会提供类似性能。理论上可以证明，在一些假设条件下，组合预测的方差减少到原方差的1/n（n: 分类器的数量）。\n",
    "\n",
    "bagging模型有各种不同的实现。随机森林是其中之一，接下来我们将讨论它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9、什么是随机森林？它是如何工作的？\n",
    "\n",
    "<img align=\"right\" src=\"figures/tree14.png\">\n",
    "\n",
    "随机森林被认为是所有数学科学问题的灵丹妙药。一个有趣的注意点，当你想不出任何算法（无论什么情况），使用随机森林！\n",
    "\n",
    "随机森林是一种多功能的机器学习方法能够执行回归和分类任务。它还承担降维方法，处理缺失值，离群值以及数据探索的其它必要措施，并做一个相当好的工作。**它是一种集成学习的方法，一群弱模型相结合形成强大的模型**。\n",
    "\n",
    "### 它是如何工作的？\n",
    "\n",
    "随机森林中，我们种植多棵树与CART模型中种一棵树相反（看CART和随机森林的比较，第一部分和第二部分）。为了一个新对象基于属性进行分类，每棵树给出一个分类，我们说树为该分类“投票”。森林选择有最多票数的分类（森林中的所有树），在回归时，它取不同树输出的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"figures/tree1.jpeg\">\n",
    "### 它以下列方式工作。每棵树种植和成长如下:\n",
    "\n",
    "-    1.假设训练集的个数是N。这N个样本采取随机但有放回。此样本用于生长树的训练集。\n",
    "\n",
    "\n",
    "-    2.如果有M个输入变量，每个节点指定一个数m<M，m变量从M中随机选择出的。在这些m上最好的分裂时用来分裂节点。当我们生长森林的时候，m的值是恒定的。\n",
    "\n",
    "\n",
    "-    3.每棵树都长到最大程度，而且没有剪枝。\n",
    "\n",
    "\n",
    "-    4.通过聚合n棵树的预测值预测新数据（例如，分类的多数票，回归的平均值）。\n",
    "\n",
    "通过案例研究更详细的了解该算法，请阅读这篇文章[“随机森林概论-简明版”](https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林的优点\n",
    "\n",
    "<img align=\"right\" src=\"figures/tree20.png\">\n",
    "\n",
    "- 该算法可以解决分类和回归两种类型的问题并且在这两方面做适当的估计。\n",
    "\n",
    "- 最让我兴奋的随机森林的好处之一是，高维处理大数据集的能力。它可以处理成千上万的输入变量并识别最重要的变量，因此它被认为是降维方法之一。而且，该模型输出变量的重要性，这可以是一个非常方便的特性（在一些随机数据集）。\n",
    "- 它有一个有效的方法来估计丢失的数据，并且大部分数据丢失时保持精度。\n",
    "\n",
    "- 它平衡数据集中分类不平衡地方的错误。\n",
    "\n",
    "- 上述功能可以扩展到未标记数据，导致无监督聚类，数据视图和异常值检测。\n",
    "\n",
    "- 随机森林涉及到输入数据的重复抽样称为bootstrap抽样。这里有三分之一的数据不用于训练，可用于测试。这些被称为袋外样本。这些袋外样本的误差估计值被称为袋外错误率。通过袋外误差估计的研究，提供证据表明袋外估计与使用相同大小的测试集作为训练集一样准确。因此，使用袋外估计消除了备用测试集的必要性。\n",
    "\n",
    "\n",
    "### 随机森林的缺点\n",
    "\n",
    "- 分类它肯定工作的很好，但不及回归问题，因为它不提供精确的连续性的预测。对于回归，它不预测超出训练数据的范围，它们可能是特别噪的过拟合数据集。\n",
    "\n",
    "- 随机森林感觉就像是统计建模的黑盒子方法——你对模型几乎没有控制。你最好的——尝试不同的参数和种子！\n",
    "\n",
    "Python scikit-learn中，随机森林有常见的实现。下面我们看看R和Python中加载随机森林模型的代码：\n",
    "\n",
    "```python\n",
    "#Import Library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#use RandomForestRegressor for regression problem\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Random Forest object\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10、什么是Boosting？它是如何工作的？\n",
    "\n",
    "定义：术语‘Boosting’是指一系列将**弱学习器转化为强学习器的算法**。\n",
    "\n",
    "通过解决垃圾邮件识别这个问题来详细了解该定义：如何将一封邮件分为是否是垃圾？和其他人一样，我们最初的方法是通过以下标准识别‘垃圾邮件’和‘不是垃圾邮件’。如果：\n",
    " -   1.电子邮件只有一个图像文件（宣传图片），是垃圾邮件。\n",
    " -   2.电子邮件只有链接，是垃圾邮件。\n",
    " -   3.电子邮件正文包含句子“你赢得了XXXXXX奖金”，是垃圾邮件\n",
    " -   4.电子邮件来自我们官方域名“Analyticsvidhya.com”，不是垃圾邮件\n",
    " -   5.电子邮件来自已知源，不是垃圾邮件\n",
    "\n",
    "以上，我们已经定义了多个规则来分类一封邮件是“垃圾邮件”或者“不是垃圾邮件”。但是，你认为这些规则单独地是否足够强大到足以成功地分类一封邮件？不能。\n",
    "\n",
    "这些规则没有强大到足以将一封邮件分类为‘垃圾邮件’或者‘不是垃圾邮件’。因此，这些规则被称为弱学习器。\n",
    "为了将弱学习器转化为强学习器，我们会使用像下面的方法，组合每个弱学习器的预测值：\n",
    "\n",
    "- 使用平均值/加权平均值\n",
    "\n",
    "- 考虑具有较高投票的预测\n",
    "\n",
    "例如：以上，我们定义了5个弱学习器。这5个中，有3个投票是‘垃圾邮件’，2个投票‘不是垃圾邮件’。在这种情况下，默认，我们会考虑一封电子邮件是垃圾邮件因为’垃圾邮件‘的投票更高（3）。\n",
    "\n",
    "#### 它是如何工作的？\n",
    "\n",
    "现在我们知道，boosting组合弱学习器又称基学习器形成一个强规则。你脑海中应该立即提出一个问题，“boosting如何识别弱规则？”\n",
    "\n",
    "\n",
    "为了寻找弱规则，我们应用有不同分布的基学习（ML）算法。每次应用基学习算法，它产生一个新弱预测规则。这是一个迭代的过程。经过多次迭代，boosting算法将这些弱规则组合成一个强预测规则。\n",
    "\n",
    "另一个问题可能会困扰你，‘我们如何为每一轮选择不同的分布？’\n",
    "\n",
    "为了选择正确的，**这里有以下步骤**：\n",
    "- 步骤1：基学习器需要所有的分布，每个观察分配相等的权重或关注。\n",
    "- 步骤2：如果第一基学习算法引起任何预测误差，那么我们更关注有预测误差的观测。然后，我们应用下一个基学习算法。\n",
    "- 步骤3：重复步骤2直到达到基学习算法的限制或达到更高精度。\n",
    "\n",
    "最后，它组合弱学习器的输出，并创建一个最终提升模型预测能力的强学习器。Boosting更高关注分类错误或通过前面的弱规则有更高错误的例子。\n",
    "\n",
    "本教程中，我们将学习两种最常用的算法，即Gradient Boosting （GBM）和XGboost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11、GBM和Xgboost哪个更强大？\n",
    "\n",
    "我一直很欣赏boostring的能力—xgboost算法。与GBM的实现相比，有时它提供了更好的结果，有时收获可有可无。当我更多地探索它的性能和高精度背后的科学，我发现很多Xgboost相对于GBM的优势：\n",
    "\n",
    "####    1.正则化\n",
    "\n",
    "- 标准的GBM实现没有像XGBoost的正则化，正则化有助于减少过拟合。\n",
    "\n",
    "- 实际上，XGBoost也被称为‘正则化boosting’技术。\n",
    "\n",
    "####    2.并行处理\n",
    "\n",
    "- XGBoost实现并行处理，相比GBM飞快。\n",
    "\n",
    "- 不过，众所周知，boosting是顺序处理的，它怎么可能并行呢？每棵树的构造在前一棵树之后，因此什么阻止我们使用所有的内核构造一棵树？我希望你理解了这句话的意思。看看这个[链接](http://zhanpengfang.github.io/418home.html)，进一步探索。\n",
    "\n",
    "-  Hadoop上也支持XGBoost实现。\n",
    "\n",
    "####    3.高灵活性\n",
    "\n",
    "- XGBoost允许用户定义自定义优化目标和评定标准。\n",
    "\n",
    "- 这增加了一个全新的维度，我们的处理不会受到任何限制。\n",
    "\n",
    "####    4.处理缺失值\n",
    "\n",
    "- XGBoost有内置的缺失值处理程序。\n",
    "\n",
    "- 用户需要提交与观测值不同的值，该值作为参数传递。XGBoost在每个节点遇到缺失值时会尝试不同的事情，并学习以后如何处理缺失值。\n",
    "\n",
    "####    5.剪枝：\n",
    "\n",
    "- 当分裂遇到负损失时，GBM将停止分裂节点。因此，它是一个贪婪算法。\n",
    "\n",
    "- XGBoost分裂到指定的max_depth，反向剪枝，去掉不再有正值的分裂。\n",
    "\n",
    "- 这种做法的另一个优点是，有时分裂负损失如-2后面有个正损失+10。GBM在-2处停下，XGBoost将继续分裂，它将发现这两个分裂共同影响+8，保留这两个。\n",
    "\n",
    "####    6.内置交叉验证\n",
    "\n",
    "- XGBoost允许在boosting处理中每轮迭代进行交叉验证，因此，很容易得到boosting迭代单次运行的最佳数。\n",
    "\n",
    "- 而GBM使用网格搜索，只能检测有限个值。\n",
    "\n",
    "####    7.在已有模型上继续\n",
    "\n",
    "- XGBoost模型可以从上一轮的结果上继续训练。这在某些特定的应用上是一个显著的优势。\n",
    "\n",
    "- sklearn中的GBM实现也有这个功能，两种算法在这一点是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12、Python中使用GBM\n",
    "\n",
    "在开始之前，快速地了解该算法的重要参数和工作原理。这对R和Python用户都有帮助。下面是GBM算法2个类的伪代码：\n",
    "\n",
    "```\n",
    "1. Initialize the outcome\n",
    "2. Iterate from 1 to total number of trees\n",
    "  2.1 Update the weights for targets based on previous run (higher for the ones mis-classified)\n",
    "  2.2 Fit the model on selected subsample of data\n",
    "  2.3 Make predictions on the full set of observations\n",
    "  2.4 Update the output with current results taking into account the learning rate\n",
    "3. Return the final output.\n",
    "```\n",
    "\n",
    "这是一个非常简单（也许幼稚）的GBM工作说明。它有助于每一个初学者了解这个算法。\n",
    "\n",
    "**Python中，提高模型性能的GBM参数：**\n",
    "####    1.learning_rate \n",
    "\n",
    "- 该参数决定每棵树最终结果（步骤2.4）的影响。GBM设定了初步估计值，用每棵树的输出更新该值。learning参数控制这种变化的幅度。\n",
    "\n",
    "- 较低的值通常是首选，因为它们使模型更健壮针对树的具体特征，从而更好地概况。\n",
    "\n",
    "- 较低的值需要更多的树来建模所有的关系，并将需要大量计算。\n",
    "\n",
    "####    2.n_estimators\n",
    "\n",
    "- 模型化的序列树的个数（步骤2）\n",
    "\n",
    "- 虽然GBM在有很多树时依然稳健，但还是可能过拟合。应该用CV调整一个特定的学习速率。\n",
    "\n",
    "####    3.subsample\n",
    "\n",
    "- 每棵树选择的部分观察值。随机采样的。\n",
    "\n",
    "- 用稍小于1的值通过减少方差使模型更稳健。\n",
    "\n",
    "- 典型值~0.8就可以，可以进一步微调。\n",
    "\n",
    "\n",
    "**除此之外，还有某些影响整体功能的其它参数**：\n",
    "\n",
    " ####   1.loss \n",
    "\n",
    "- 每个分裂中被最小化的损失函数。\n",
    "\n",
    "- 分类和回归中它有不同的值。一般默认值就可以。只有当你理解它们对模型的影响时，才选择其它值。\n",
    "\n",
    "####    2.init    \n",
    "\n",
    "- 该参数影响输出的初始化。\n",
    "\n",
    "- 如果我们有其他模型，其结果作为GBM初步估计，这时就可以用init。\n",
    "\n",
    "####    3.random_state\n",
    "\n",
    "- 随机数种子，每次产生相同的随机数。\n",
    "\n",
    "- 对于参数调优重要。如果不固定这个随机数，后续相同的参数运行得到不同的结果，这样很难比较模型。\n",
    "\n",
    "- 一个特定的随机样本可能导致过拟合。可以尝试用不同随机样本运行模型，这需要大量计算，因而一般不采用。\n",
    "\n",
    "####    4.verbose\n",
    "\n",
    "- 建模完成时打印的输出类型。取值如下：\n",
    "```\n",
    "0：无输出（默认）\n",
    "1：某些区间的树输出\n",
    ">1：所有树输出\n",
    "```\n",
    "\n",
    "####    5.warm_start\n",
    "\n",
    "- 这个参数用起来有趣，公正地使用可以有很大的帮助。\n",
    "\n",
    "- 使用它可以用一个建好的模型来训练额外的树。这可以节省大量时间，对于高级应用应该探索这个选项。\n",
    "\n",
    "####    6.presort\n",
    "\n",
    "- 为了更快分裂，选择是否对数据进行预排序。\n",
    "\n",
    "- 默认自动选择，如果需要可以更改。\n",
    "\n",
    "### GBM in Python\n",
    "\n",
    "```python\n",
    "#import libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "\n",
    "#use GBM function\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13、在Python中使用XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) 是gradient boosting算法的先进实现。并行计算的特点使它比现有的gradient boosting实现至少快10倍。它支持各种目标函数，包括回归，分类和排名。\n",
    "\n",
    "Python教程：这是针对Python用户的XGBoost的全面教程。查看[教程](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Where to practice ?\n",
    "\n",
    "Practice is the one and true method of mastering any concept. Hence, you need to start practicing if you wish to master these algorithms.\n",
    "\n",
    "Till here, you’ve got gained significant knowledge on tree based models along with these practical implementation. It’s time that you start working on them. Here are open practice problems where you can participate and check your live rankings on leaderboard:\n",
    "\n",
    "For Regression: [Big Mart Sales Prediction](https://datahack.analyticsvidhya.com/contest/practice-problem-bigmart-sales-prediction/)\n",
    "\n",
    "For Classification: [Loan Prediction](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尾注\n",
    "\n",
    "基于树的算法对每个数据科学家学习很重要。实际上，树模型号称在所有机器学习算法中提供最佳性能。本教程学习了GBM和XGBoost，并以此为止。\n",
    "\n",
    "我们讨论srcatch的树模型。得知决策树的重要性和简化概念如何用于boosting算法。建议在实际中继续练习这些算法。注意boosting算法相关参数。希望本教程在树模型方面全面的知识充实你。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
