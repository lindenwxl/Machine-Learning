{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习算法基础\n",
    "\n",
    "## 介绍\n",
    "``Note: This article was originally published on Aug 10, 2015 and updated on Sept 9th, 2017``\n",
    "\n",
    "*Google’s self-driving cars and robots get a lot of press, but the company’s real future is in machine learning, the technology that enables computers to get smarter and more personal.*\n",
    "\n",
    " – **Eric Schmidt (Google Chairman)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可能正生活在人类历史上最有代表性的时代。这是一个计算从大型主机变迁到个人电脑，再到云的时代。但是使这个时具有代表性的不是已经发生了的事，而是未来几年将要发生的事。\n",
    "\n",
    "令一个像我这样的人对这个时代感到兴奋的是，随着计算力的增强而来的工具与技术的民主化。今天，作为一个数据科学家，我可以仅以几美元每小时的代价搭建起一台运行着复杂算法的大规模数据处理机。但是，要达到这个水平不容易，我经历了我自己的黑暗时期。\n",
    "\n",
    "We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to cloud. But what makes it defining is not what has happened, but what is coming our way in years to come.\n",
    "\n",
    "What makes this period exciting for some one like me is the democratization of the tools and techniques, which followed the boost in computing. Today, as a data scientist, I can build data crunching machines with complex algorithms for a few dollors per hour. But, reaching here wasn’t easy! I had my dark days and nights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我今天给出的或许是我至今为止创作的最有价值的指南\n",
    "\n",
    "创作本指南背后的想法是简化有志向的数据科学家和机器学习爱好者探索世界的旅程。通过本指南，我会让你能够着手解决机器学习问题，并从经验中学习。我会给出几个机器学习算法的高层次的解释，还有实现它们的R和Python代码。这已经足够让你上手了。\n",
    "\n",
    "The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world. Through this guide, I will enable you to work on machine learning problems and gain from experience. I am providing a high level understanding about various machine learning algorithms along with R & Python codes to run them. These should be sufficient to get your hands dirty.\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我刻意忽略的这些技术背后的统计学知识，因为刚开始时你不必理解这些知识。所以，如果你要理解这些技术背后的统计学思想，你就要到其他地方去找了。但是，如果你只想尽快开始构建一个机器学习项目，那就来对地方了。\n",
    "\n",
    "I have deliberately skipped the statistics behind these techniques, as you don’t need to understand them at the start. So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. But, if you are looking to equip yourself to start building machine learning project, you are in for a treat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广义上说，存在3种类型的机器学习算法...\n",
    "\n",
    "### 1、监督学习(Supervised Learning)\n",
    "\n",
    "工作原理：这类算法由一集给定的预示变量（独立变量）中预测得到的一个目标变量（依赖变量）组成。使用这些变量集，我们得到一个从输入映射到输出的函数。训练过程持续到模型能在训练集上达到指定的精度。监督学习的例子有：``回归、决策树、随机森林、KNN算法、逻辑回归等``。\n",
    "\n",
    "How it works: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.\n",
    "\n",
    "### 2、非监督学习(Unsupervised Learning)\n",
    "\n",
    "工作原理：在这类算法里，我们没有要预测的目标变量。这类算法被用于族群的分类，特别是划分客户群用于进一步研究。非监督学习的例子： Apriori算法、K平均数算法。\n",
    "\n",
    "How it works: In this algorithm, we do not have any target or outcome variable to predict / estimate.  It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means.\n",
    "\n",
    "### 3、强化学习(Reinforcement Learning)\n",
    "\n",
    "工作原理：使用这类算法，机器被训练用来做某种特定的决定。它是这样工作的：机器被放在一个允许他不断试错的环境下，机器从过去的经验中学习，并尝试获得作出准确决定所需的最好的知识。强化学习的例子有：Markov决策过程。\n",
    "\n",
    "How it works:  Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision Process.\n",
    "\n",
    "\n",
    "### 常用机器学习算法列表\n",
    "\n",
    "下面是经常使用的机器学习算法的一个列表。这些算法几乎可以用于任何数据问题。\n",
    "\n",
    "- 1、线性回归:Linear Regression\n",
    "\n",
    "- 2、逻辑回归:Logistic Regression\n",
    "\n",
    "- 3、决策树:Decision Tree\n",
    "\n",
    "- 4、SVM算法:SVM\n",
    "\n",
    "- 5、朴素贝叶斯算法:Naive Bayes\n",
    "\n",
    "- 6、KNN算法:kNN\n",
    "\n",
    "- 7、K平均数算法:K-Means\n",
    "\n",
    "- 8、随机森林算法:Random Forest\n",
    "\n",
    "- 9、降维算法:Dimensionality Reduction Algorithms\n",
    "\n",
    "- 10、Gradient Boost 和 Adaboost 算法:Gradient Boosting algorithms\n",
    "- (1)GBM\n",
    "- (2)XGBoost\n",
    "- (3)LightGBM\n",
    "- (4)CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、线性回归\n",
    "\n",
    "这种算法常用于在连续变量的基础上估计实际值（房价、呼叫量、总销售额等）。通过在独立变量和依赖变量间拟合一条直线来建立独立变量和依赖变量间的关系。最佳拟合线叫做回归线，可由线性方程$Y=a*X+b$来表示。\n",
    "\n",
    "理解线性回归最好的方法是回想一下童年经历。假如，你要一个5年级小孩依据体重对他班上的同学升序排列，但又不让他问同学的体重。你认为这个小孩会怎么做？他可能会观察（视觉分析）同学的身高和体型，然后组合这些可以看得到的参数对他们排序。这就是线性回归的实际应用。这个小孩实际上想到了身高和体型会和体重有关联，而这就如同上面介绍的方程.\n",
    "\n",
    "在这个方程里：\n",
    "\n",
    "- Y——依赖变量\n",
    "- a——斜率\n",
    "- X——独立变量\n",
    "- b——截距\n",
    "\n",
    "参数是由最小化数据点到回归线的距离的平方差之和推导而来的\n",
    "\n",
    "*线性回归主要有两种类型：一元线性回归和多元线性回归。一元线性回归的特点是只有一个独立变量。而多元线性回归（就像名字暗示的那样）的特点是有多个（大于1）独立变量。为了找到最佳回归线，你可以拟合成多项式或者曲线，而这就叫做``多项式回归和曲线回归``。*\n",
    "\n",
    "看下面的例子。我们得到了最佳拟合线的方程为：y=0.2811x+13.9。现在使用这个方程，我们就能由一个人的身高推得他的体重。\n",
    "<img align=\"left\" src=\"figures/machine2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [-0.3368496  -0.07720281  0.51016731  0.31996167]\n",
      "Intercept: \n",
      " 0.871087079938\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "linear = linear_model.LinearRegression()\n",
    "# Train the modle using the training sets and check score\n",
    "linear.fit(x_train, y_train)\n",
    "linear.score(x_train, y_train)\n",
    "\n",
    "#Equation coefficient and Intercept\n",
    "print('Coefficient: \\n', linear.coef_)\n",
    "print('Intercept: \\n', linear.intercept_)\n",
    "\n",
    "# Predict Output\n",
    "predicted = linear.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、逻辑回归\n",
    "\n",
    "别被算法名字弄糊涂了。本算法是一个分类算法而非回归算法。它用于在给定独立变量的基础上估计离散值（二值，像0与1，是与否，真与假）。简单来说，这就是通过拟合数据到一个逻辑函数来预测一个事件发生的可能性。也因此被称作逻辑回归。因为他预测的是概率，所以输出变量就在0与1之间。\n",
    "\n",
    "像上面一样，我们试试用一个简单的例子来理解该算法。\n",
    "\n",
    "假设你朋友给你个谜题要你解。只有两种结果情形：你要么解出来了，要么没解出来。现在设想，给你出了范围广泛的各种谜题，用来看看你擅长哪些领域。结果最后会是这样：如果你面临的是10年级的三角几何踢，你有70%的概率解出来，如果是5年级的历史题，你答对的概率只有30%。这就是逻辑回归给你提供的回答。\n",
    "\n",
    "回到数学上，输出几率的对数被建模成一个预示变量的线性组合\n",
    "\n",
    "```\n",
    "odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence\n",
    "ln(odds) = ln(p/(1-p))\n",
    "logit(p) = ln(p/(1-p)) = b0+b1*X1+b2*X2+b3*X3....+bk*Xk\n",
    "```\n",
    "\n",
    "上面，p是我们感兴趣的特征出现的概率。选参数要选最大化观测样本值出现的概率的参数，而不是最小化方差和（如传统回归里那样）的参数。\n",
    "\n",
    "现在你可能要问，为什么要取对数？为了简化起见。现在我只能说这是重现阶梯函数的最佳方法之一。更深入的细节已经超出了本文的目的了。\n",
    "\n",
    "<img align=\"left\" src=\"figures/machine3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[ 0.30321778  0.81728511 -1.26157325 -0.58731511]\n",
      " [-0.06289797 -0.70452245  0.48247094 -0.11795415]\n",
      " [-0.62754927 -0.56995235  0.90931458  0.73224196]]\n",
      "Intercept: \n",
      " [ 0.21257149 -0.24081087 -0.13304209]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = LogisticRegression()\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "#Equation coefficient and Intercept\n",
    "print('Coefficient: \\n', classfier.coef_)\n",
    "print('Intercept: \\n', classfier.intercept_)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下一步...\n",
    "\n",
    "可以尝试有很大不同的方法来改进本模型：\n",
    "\n",
    "- 添加交互项\n",
    "\n",
    "- 移除特征项\n",
    "\n",
    "- 正则化技术\n",
    "\n",
    "- 使用非线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、决策树\n",
    "\n",
    "这是我最喜欢的算法之一，而且我也经常使用这个算法。这是一类主要用于问题分类的监督学习算法。神奇的是，该算法即适合确定性依赖变量，也适用于连续性依赖变量。在这个算法里，我们将总体分成几个同类集，分法是基于最显著的属性或独立变量的，为的是使各个类之间尽可能有明显区别。更多细节，你可以阅读[简化决策树](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)一文。\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上图，你可以看到，为了确定“他们是否会出去玩”，总体在基于多个属性的情况下，被分成了4个不同的类别。为了将全体分类成各种不同的类别，会使用多种不同的技术，如Gini、Information Gain、Chi-square、entropy.\n",
    "\n",
    "理解决策树工作原理最好的方法是看一看玩Jezzball游戏——一个来自微软的经典游戏（见下图）。基本上，你有一个可以移动墙的房间，你需要造墙，使没有球的空间最大化。\n",
    "\n",
    "所以，每次你用一个墙分割房间，你都相当于在一个房间里创造了两个不同的类别。决策树的做法很像这种将总体分成尽可能不同的类别的风格。\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine3.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948148148148\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = tree.DecisionTreeClassifier(criterion='gini')\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、SVM（支持向量机）\n",
    "\n",
    "这也是一个分类算法。在本算法里，我们将每个数据项画在一个N维空间里（N是数据项的特性数），数据项的每个特性对应一个坐标。\n",
    "\n",
    "例如，假设我们只有两个特性，比如个体的身高和头发长度。我们先将这两个变量画成一个二维空间，空间里的每个点都有一个二维坐标（这些坐标就是支持向量）。\n",
    "\n",
    "现在，我们要找一条将数据分割成两个不同类别的线。这条线将使得两个类别中离该线最近的的点到该线的距离最大化。\n",
    "\n",
    "在显示的例子里。分割数据成两个不同类别的线是那条黑线。因为这时离线最近的两点（分别来自两个不同类别）到线的距离最大。这条线就是我们的分类器。然后，测试数据落到该线的哪一边，它就被分到哪一类。\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine4.png\"><img align=\"right\" src=\"figures/Machine5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948148148148\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = svm.SVC()\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想象这个算法是在N维空间里网JezzBall游戏。游戏规则稍稍改成：\n",
    "\n",
    "- 你可以以任意角度画线或者面（而不是像原游戏中那样，只允许水平的或竖直的）\n",
    "\n",
    "- 游戏目标变为将不同颜色的球分隔在不同的房间\n",
    "\n",
    "- 球不允许动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、朴素贝叶斯算法\n",
    "\n",
    "本算法是一个基于假设各个预示变量相互独立时的贝叶斯理论的分类算法。简单来说，朴素贝叶斯分类器假设一个类别中的某特定特性和其他特性不相关。举例来说，一个水果可能被认为是一个苹果，如果它是红色的，圆的，而且直径大概3英寸。即使这些特性彼此相关，或者依赖于其他特性，朴素贝叶斯分类器也会认为所有这些特性在对那水果是苹果的概率的贡献上是相互独立的，\n",
    "\n",
    "朴素贝叶斯模型对大型数据来说很容易构建，而且非常有用。除了它的简明性，朴素贝叶斯模型还因为它甚至比一些成熟的分类方法还有效而出名。\n",
    "\n",
    "贝叶斯理论提供了一种从P(c)，P(x)和P(x|c)计算P(c|x)的方法。看下面的等式：\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine6.jpg\">\n",
    "\n",
    "## 这里：\n",
    "\n",
    "- P(c|x)是已知预示变量（属性）的情况下，类（目标）的后验概率\n",
    "\n",
    "- P(c)是类的先验概率\n",
    "\n",
    "- P(x|c)是给定类的前提下，预示变量的概率\n",
    "\n",
    "- P(x)是预示变量的先验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``例``**：让我们用一个实例来理解。下面是一个由天气变量和与之相对的目标变量“出去玩”构成的训练数据集。现在我们需要基于天气来确定玩家是否“出去玩”。让我们做以下操作：\n",
    "\n",
    "- 步骤1：将数据集转换为频率表\n",
    "- 步骤2：通过找到像阴天（Overcast）概率为0.29、“出去玩”的概率是0.64这样的值来构造概率表\n",
    "- 步骤3：现在，通过朴素贝叶斯等式技术每类的后验概率。每类中具有最高后验概率值的就是输出\n",
    "\n",
    "<img align=\"left\" src=\"figures/Machine6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：这个陈述：玩家在天气晴朗时会出去玩，是正确的吗？\n",
    "\n",
    "使用我们已经讨论过的方法， P（会玩 | 晴朗）= P（晴朗 | 会玩）* P（会玩）/ P （晴朗）。这里我们有P （晴朗 |会玩）= 3/9 = 0.33，P（晴朗） = 5/14 = 0.36, P（会玩）= 9/14 = 0.64。所以，P(会玩 | 晴朗）= 0.33 * 0.64 / 0.36 = 0.60，这是个更大的概率。\n",
    "\n",
    "朴素贝叶斯会基于不同的属性，使用类似的方法来预测不懂类别的概率。这个算法常用于文本分类和其他涉及多个类的问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940740740741\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = GaussianNB()\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、KNN算法（K-最近邻算法）\n",
    "\n",
    "这个算法既可以用于分类问题，也可以用于回归问题。然而，在业界常更广泛用于分类问题。KNN算法是一个简单的算法，它存储所有案例，并依据新案例边的K个邻居的主要情况将新案例分类。一个案例将会被分到一个类中，这个类会是这个案例在某个距离函数视角下它的K个最近邻居们最常出现于的类。\n",
    "\n",
    "这个距离函数可以是欧几里得距离，曼哈顿距离，明氏距离或汉明距离。前三个可以用于连续函数，第四个可以用于确定性变量。如果K=1，那么情况就简化到只考虑最近邻。有时候，选择合适的K值会是进行KNN建模的一大挑战\n",
    "\n",
    "KNN算法可以很容易联系到我们的现实生活中来。如果你想了解一个你从来都没有接触过的人，你也许会从他最亲密的朋友，他生活的圈子来获得有关他的信息。\n",
    "\n",
    "选择KNN算法之前应该考虑的事：\n",
    "\n",
    "- KNN是计算密集型的算法\n",
    "\n",
    "- 变量应该要标准化，否则范围大的变量会对结果产生影响\n",
    "\n",
    "- 进行KNN算法之前，要多注意数据的预处理阶段，比如偏差数据剔除，噪声过滤等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = KNeighborsClassifier(n_neighbors = 5 )\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、K-平均数算法\n",
    "\n",
    "这是一个非监督算法。解决的是分类问题。它采用简单容易的方法将给定数据分成特定数量的簇（假设的K个簇）。在一个簇中的数据点都是同类的，并且都异于其他簇中的点。\n",
    "\n",
    "记得识别墨水点的例子吗？K-平均数算法有点像这个过程。你看着墨水点，并尝试分辨到底有多少个不同的簇或群。\n",
    "\n",
    "<img align=\"left\" src=\"figures/machine7.png\">\n",
    "\n",
    "#### K-平均数算法怎样形成簇：\n",
    "\n",
    "- 1、k-平均数算法为每个簇（共K个）选择一个中心点。\n",
    "\n",
    "- 2、每个数据点同其最近的中心点形成一个簇。\n",
    "\n",
    "- 3、基于现有的簇，找到每个簇现在的中心点。\n",
    "\n",
    "- 4、利用新的中心点，重复2和3，找到每个数据点同新的中心点的最近距离，新成新的K个簇，重复这一过程直到出现聚合，即中心点不变。\n",
    "\n",
    "#### 如何决定K的值：\n",
    "\n",
    "在K-平均数算法中，我们有一些簇，每个簇都有自己的中心点。一个簇中的各点同中心点的差的平方和构成了这个簇的平方和值。所有簇的平方和值相加就是这个分簇方案的总平方和值。\n",
    "\n",
    "我们知道，随着簇的数量的增加，这个值（总平方和值）是递减的。但如果你将簇数与相应的总平方和值画在图里就会发现，在某个K值之前总平方和值下降的很快，而过了这个值后下降的就变缓了。这个K值，就是我们认为的最佳簇数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777777777778\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = KMeans(n_clusters=3, random_state=5)\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、随机森林算法\n",
    "\n",
    "随机森林是表示决策树集合的标志性名词。在随机森林中，我们有决策树集（也叫作森林）。为了基于新对象的属性来分类该对象，每颗树（针对该对象）都给出了一个分类，我们说这棵树“赞成”该分类。森林选择多数树赞成的分类。\n",
    "\n",
    "每颗树都按以下方式培育：\n",
    "- 1、如果训练集的案例数是N，那么使用重置随机抽样N次产生一个样本，该样本将会作为决策树的训练集。\n",
    "\n",
    "- 2、如果有M个输入变量，那就指定一个数m（m<<M）使得对每个节点，m个变量都从M个输入变量中随机选取，而且让这m个变量的分法应用于分类节点。m的值在整个森林生长过程中不变。\n",
    "\n",
    "- 3、每棵树都长到最大，全程不剪枝。\n",
    "\n",
    "更多关于这个算法的细节，请比较决策树和优化模型参数。我推荐你读一读下面几篇文章：\n",
    "\n",
    "- 1、[随机森林介绍——简化版](https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/)\n",
    "\n",
    "- 2、[CART模型与随机森林比较（第一部分)](https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/)\n",
    "\n",
    "- 3、[CART模型与随机森林比较（第二部分）](https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/)\n",
    "\n",
    "- 4、[随机森林模型的参数调优](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925925925926\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "# Import other necessary libararies like pandas , numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Train and Test datasets\n",
    "# Identify feature and response variables and values must\n",
    "# be numeric and numpy arrays\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= .9)\n",
    "\n",
    "# Create linear regression objects\n",
    "classfier = RandomForestClassifier()\n",
    "# Train the modle using the training sets and check score\n",
    "classfier.fit(x_train, y_train)\n",
    "classfier.score(x_train, y_train)\n",
    "\n",
    "# Predict Output\n",
    "predicted = classfier.predict(x_test)\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9、降维算法\n",
    "\n",
    "过去的4-5年中，各个领域中收集到的数据都呈指数级增长。政府和机构不仅有新的数据来源，而且获得的数据也更详细。\n",
    "\n",
    "例如，电子商务公司捕捉到了像客户群特征、网站浏览历史、他们的喜好、购买历史等这样的更详细的客户数据，使电子商务公司能比他们（客户们）最近的杂货店主都更准确的给予客户个性化的关注。\n",
    "\n",
    "作为一个数据科学家，提供给我们的数据也包含了很多特征，这听起来对构建健壮模型是个好消息。但其中也存在着挑战。你如何从1000或2000个变量中识别出那些关键的变量呢？在这里，降维算法和其他的像决策树算法，随机森林算法，PCA算法，因子分析法一起，在相关矩阵、缺失值比例等基础上帮我们确定（关键变量）。\n",
    "\n",
    "要了解更多关于本算法的介绍，请阅读[“学习降维技术的初学者指南”](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/)。\n",
    "\n",
    "```python\n",
    "#Import Library\n",
    "from sklearn import decomposition\n",
    "#Assumed you have training and test data set as train and test\n",
    "# Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)\n",
    "# For Factor analysis\n",
    "#fa= decomposition.FactorAnalysis()\n",
    "# Reduced the dimension of training dataset using PCA\n",
    "train_reduced = pca.fit_transform(train)\n",
    "#Reduced the dimension of test dataset\n",
    "test_reduced = pca.transform(test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10、Gradient Boosting和AdaBoost算法\n",
    "\n",
    "GBM和AdaBoost算法都是boosting算法，用于处理大量数据，作出高精度预测。Boosting是一种聚集算法，它在几个基础估算子的基础上决策，相对于单一估算子，它提高了健壮性。它组合多个弱的或一般的预测器形成一个强预测器。这些boosting算法在数据科学竞赛（像Kaggle，AV Hackathon，CrowdAnalytix）中表现的总是很好\n",
    "\n",
    "更多的请查看：[详细了解Gradient Boosting和AdaBoost算法](https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/)。\n",
    "\n",
    "```python\n",
    "#Import Library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Gradient Boosting Classifier object\n",
    "model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记\n",
    "\n",
    "到现在，我确定你已经有了常用机器学习算法的概念了。我写本文并提供R和Python代码的唯一目的就是让你可以快速开始。如果你热切的想掌握机器学习，立即开始吧。带着问题，获得（应用这些代码）过程中的具体理解，并体会其中的乐趣吧。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
