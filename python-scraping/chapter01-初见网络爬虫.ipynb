{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初见网络爬虫\n",
    "\n",
    "本章将首先向网络服务器发送GET 请求以获取具体网页，再从网页中读取HTML 内容，\n",
    "最后做一些简单的信息提取，将我们要寻找的内容分离出来。\n",
    "\n",
    "1.0 向网络服务器发送GET请求以获取具体网页\n",
    "\n",
    "2.0 从网页中读取HTML内容\n",
    "\n",
    "3.0 信息提取、分离寻找的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 网络链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "# Python实现方式\n",
    "\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "%run scrapetest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将会输出http://pythonscraping.com/pages/page1.html 这个网页的全部HTML 代码。更\n",
    "准确地说，这会输出在域名为http://pythonscraping.com 的服务器上< 网络应用根地址>/\n",
    "pages 文件夹里的HTML 文件page1.html 的源代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**urllib 是Python 的标准库（就是说你不用额外安装就可以运行这个例子），包含了从网\n",
    "络请求数据，处理cookie，甚至改变像请求头和用户代理这些元数据的函数。我们将在\n",
    "本书中广泛使用urllib，所以建议你读读这个库的Python 文档（https://docs.python.org/3/\n",
    "library/urllib.html）。**\n",
    "\n",
    "\n",
    "**urlopen 用来打开并读取一个从网络获取的远程对象。因为它是一个非常通用的库（它可\n",
    "以轻松读取HTML 文件、图像文件，或其他任何文件流），所以我们将在本书中频繁地使\n",
    "用它。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BeautifulSoup简介\n",
    "\n",
    "BeautifulSoup 尝试化平淡为神奇。它通过定位HTML 标签来\n",
    "格式化和组织复杂的网络信息，用简单易用的Python 对象为我们展现XML 结构信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup 库最常用的对象恰好就是BeautifulSoup 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "bsObj = BeautifulSoup(html.read(),'html.parser')#  BeautifulSoup(YOUR_MARKUP, \"lxml\")  markup_type=markup_type))\n",
    "print(bsObj.h1)\n",
    "print(bsObj.html.h1)\n",
    "print(bsObj.html.body.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们导入urlopen，然后调用html.read() 获取网页的HTML 内容。这\n",
    "样就可以把HTML 内容传到BeautifulSoup 对象，转换成下面的结构：\n",
    "\n",
    "```html\n",
    "\n",
    "html → <html><head>...</head><body>...</body></html>\n",
    "— head → <head><title>A Useful Page<title></head>\n",
    "— title → <title>A Useful Page</title>\n",
    "— body → <body><h1>An Int...</h1><div>Lorem ip...</div></body>\n",
    "— h1 → <h1>An Interesting Title</h1>\n",
    "— div → <div>Lorem Ipsum dolor...</div>\n",
    "```\n",
    "\n",
    "\n",
    "我们从网页中提取的``<h1> ``标签被嵌在BeautifulSoup 对象bsObj 结构的第二层\n",
    "（html → body → h1）。但是，当我们从对象里提取h1 标签的时候，可以直接调用它\n",
    "\n",
    "```\n",
    "bsObj.h1\n",
    "```\n",
    "\n",
    "其实，下面的所有函数调用都可以产生同样的结果：\n",
    "```html\n",
    "bsObj.html.body.h1\n",
    "bsObj.body.h1\n",
    "bsObj.html.h1.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在第3 章，我们将进一步探讨一些更复杂的BeautifulSoup 函数，还会介绍正则表达式，以\n",
    "及如何把正则表达式用于BeautifulSoup 以对网站信息进行提取**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 可靠的网络链接\n",
    "\n",
    "为什么一开始不估计可能会出现的异常！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看爬虫import 语句后面的第一行代码，如何处理那里可能出现的异常：\n",
    "\n",
    "``html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这行代码主要可能会发生两种异常：\n",
    "- 网页在服务器上不存在（或者获取页面的时候出现错误）\n",
    "- 服务器不存在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一种异常发生时，程序会返回HTTP 错误\n",
    "\n",
    "HTTP 错误可能是“404 Page Not Found”“500 Internal Server Error”等。所有类似情形，urlopen 函数都会抛出“HTTPError”异常。我们\n",
    "可以用下面的方式处理这种异常:\n",
    "```python\n",
    "try:\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "except HTTPError as e:\n",
    "print(e)\n",
    "# 返回空值，中断程序，或者执行另一个方案\n",
    "else:\n",
    "# 程序继续。注意：如果你已经在上面异常捕捉那一段代码里返回或中断（break），\n",
    "# 那么就不需要使用else语句了，这段代码也不会执行\n",
    "```\n",
    "\n",
    "如果程序返回HTTP 错误代码，程序就会显示错误内容，不再执行else 语句后面的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果服务器不存在（``就是说链接http://www.pythonscraping.com/ 打不开，或者是URL 链接写错了``），urlopen 会返回一个None 对象。\n",
    "\n",
    "这个对象与其他编程语言中的null 类似。我们可以增加一个判断语句检测返回的html 是不是None：\n",
    "```python\n",
    "if html is None:\n",
    "print(\"URL is not found\")\n",
    "else:\n",
    "# 程序继续\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，即使网页已经从服务器成功获取，如果网页上的内容并非完全是我们期望的那样，\n",
    "仍然可能会出现异常.**每当你调用BeautifulSoup 对象里的一个标签时，增加一个检查条件\n",
    "保证标签确实存在是很聪明的做法**。如果你想要调用的标签不存在，``BeautifulSoup`` 就会返回``None ``对象。不过，如果再调用这个None 对象下面的子标签，就会发生``AttributeError``错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这行代码（nonExistentTag 是虚拟的标签，BeautifulSoup 对象里实际没有）:\n",
    "```python\n",
    "print(bsObj.nonExistentTag)\n",
    "```\n",
    "会返回一个None 对象。处理和检查这个对象是十分必要的。如果你不检查，直接调用这个\n",
    "None 对象的子标签，麻烦就来了。如下所示。\n",
    "```python\n",
    "print(bsObj.nonExistentTag.someTag)\n",
    "```\n",
    "\n",
    "这时就会返回一个异常：\n",
    "\n",
    "```\n",
    "AttributeError: 'NoneType' object has no attribute 'someTag'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**那么我们怎么才能避免这两种情形的异常呢？最简单的方式就是对两种情形进行检查：**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    badContent = bsObj.nonExistingTag.anotherTag   \n",
    "except AttributeError as e:\n",
    "    print(\"Tag was not found\")\n",
    "else:\n",
    "    if badContent == None:\n",
    "        print (\"Tag was not found\")\n",
    "    else:\n",
    "        print(badContent)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初看这些检查与错误处理的代码会觉得有点儿累赘，但是，我们可以重新简单组织一下代\n",
    "码，让它变得不那么难写（更重要的是，不那么难读）。例如，下面的代码是上面爬虫的\n",
    "另一种写法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，我们创建了一个getTitle 函数，可以返回网页的标题，如果获取网页\n",
    "的时候遇到问题就返回一个None 对象。在getTitle 函数里面，我们像前面那样检查了\n",
    "HTTPError，然后把两行BeautifulSoup 代码封装在一个try 语句里面。这两行中的任何一\n",
    "行有问题，AttributeError 都可能被抛出（如果服务器不存在，html 就是一个None 对象，\n",
    "html.read() 就会抛出AttributeError）。其实，我们可以在try 语句里面放任意多行代码，\n",
    "或者放一个在任意位置都可以抛出AttributeError 的函数。\n",
    "\n",
    "在写爬虫的时候，思考代码的总体格局，让代码既可以捕捉异常又容易阅读，这是很重要\n",
    "的。如果你还希望能够很大程度地重用代码，那么拥有像getSiteHTML 和getTitle 这样的\n",
    "通用函数（具有周密的异常处理功能）会让快速稳定地网络数据采集变得简单易行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
