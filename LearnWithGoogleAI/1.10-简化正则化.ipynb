{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化正则化 (Regularization for Simplicity)\n",
    "\n",
    "**正则化**指的是降低模型的复杂度以减少过拟合\n",
    "\n",
    "- 学习目标\n",
    "  - 了解复杂度与泛化之间的权衡。\n",
    "  - 使用 L2 正则化进行实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化正则化 (Regularization for Simplicity)：L₂ 正则化\n",
    "\n",
    "请查看以下泛化曲线，该曲线显示的是训练集和验证集相对于训练迭代次数的损失:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图 1. 训练集和验证集损失**\n",
    "\n",
    "<img align=\"left\" src=\"photos/正则化1.png\">\n",
    "\n",
    "图 1 显示的是某个模型的训练损失逐渐减少，但验证损失最终增加。\n",
    "\n",
    "换言之，该泛化曲线显示该模型与训练集中的数据过拟合。\n",
    "\n",
    "根据奥卡姆剃刀定律，或许我们可以通过降低复杂模型的复杂度来防止过拟合，这种原则称为正则化。\n",
    "\n",
    "也就是说，并非只是以最小化损失（经验风险最小化）为目标:  $\\text{minimize(Loss(Data|Model))}$\n",
    "\n",
    "而是以最小化损失和复杂度为目标，这称为**结构风险最小化**:$\\text{minimize(Loss(Data|Model) + complexity(Model))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。\n",
    "\n",
    "机器学习速成课程重点介绍了两种衡量模型复杂度的常见方式（这两种方式有些相关）：\n",
    "\n",
    "- 将模型复杂度作为模型中所有特征的权重的函数。\n",
    "- 将模型复杂度作为具有非零权重的特征总数的函数。（后面的一个单元介绍了这种方法。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型复杂度是权重的函数，则特征权重的绝对值越高，模型就越复杂。\n",
    "\n",
    "我们可以使用 L2 正则化公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和：\n",
    "\n",
    "$L_2\\text{ regularization term} = ||\\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}$\n",
    "\n",
    "\n",
    "在这个公式中，接近于 0 的权重对模型复杂度几乎没有影响，而离群值权重则可能会产生巨大的影响。\n",
    "\n",
    "例如，某个线性模型具有以下权重：\n",
    "\n",
    "$\\{w_1 = 0.2, w_2 = 0.5, w_3 = 5, w_4 = 1, w_5 = 0.25, w_6 = 0.75\\}$\n",
    "\n",
    "L2 正则化项为 26.915："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_1^2 + w_2^2 + \\boldsymbol{w_3^2} + w_4^2 + w_5^2 + w_6^2$\n",
    "\n",
    "$= 0.2^2 + 0.5^2 + \\boldsymbol{5^2} + 1^2 + 0.25^2 + 0.75^2$\n",
    "\n",
    "\n",
    "$= 0.04 + 0.25 + \\boldsymbol{25} + 1 + 0.0625 + 0.5625$\n",
    "\n",
    "$= 26.915$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是 w3（上述加粗内容）的平方值为 25，几乎贡献了全部的复杂度。所有 5 个其他权重的平方和对 L2 正则化项的贡献仅为 1.915。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化正则化 (Regularization for Simplicity)：Lambda "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型开发者通过以下方式来调整正则化项的整体影响：用正则化项的值乘以名为 ``lambda``（又称为正则化率）的标量。也就是说，模型开发者会执行以下运算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{minimize(Loss(Data|Model)} + \\lambda \\text{ complexity(Model))}$\n",
    "\n",
    "执行 L2 正则化对模型具有以下影响:\n",
    "\n",
    "- 使权重值接近于 0（但并非正好为 0）\n",
    "- 使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布\n",
    "\n",
    "增加 lambda 值将增强正则化效果。 例如，lambda 值较高的权重直方图可能会如图 2 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图 2. 权重直方图**\n",
    "\n",
    "<img align=\"left\" src=\"photos/正则化2.png\">\n",
    "\n",
    "增加 lambda 值将增强正则化效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图 3. 较低的 lambda 值得出的权重直方图**\n",
    "\n",
    "<img align=\"left\" src=\"photos/正则化3.png\">\n",
    "\n",
    "降低 lambda 的值往往会得出比较平缓的直方图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图 3. 较低的 lambda 值得出的权重直方图。\n",
    "\n",
    "- 在选择 lambda 值时，目标是在简单化和训练数据拟合之间达到适当的平衡：\n",
    "\n",
    "- 如果您的 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。您的模型将无法从训练数据中获得足够的信息来做出有用的预测。\n",
    "\n",
    "- 如果您的 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。\n",
    "\n",
    "#### 注意：\n",
    "  - 将 lambda 设为 0 可彻底取消正则化。 在这种情况下，训练的唯一目的将是最小化损失，而这样做会使过拟合的风险达到最高。\n",
    "  \n",
    "理想的 lambda 值生成的模型可以很好地泛化到以前未见过的新数据。 遗憾的是，理想的 lambda 值取决于数据，因此您需要手动或自动进行一些调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  L2 正则化和学习速率\n",
    "\n",
    "学习速率和 lambda 之间存在密切关联。强 L2 正则化值往往会使特征权重更接近于 0。较低的学习速率（使用早停法）通常会产生相同的效果，因为与 0 的距离并不是很远。 因此，同时调整学习速率和 lambda 可能会产生令人混淆的效果。\n",
    "\n",
    "**早停法**指的是在模块完全收敛之前就结束训练。在实际操作中，我们经常在以在线（连续）方式进行训练时采取一些隐式早停法。也就是说，一些新趋势的数据尚不足以收敛。\n",
    "\n",
    "如上所述，更改正则化参数产生的效果可能会与更改学习速率或迭代次数产生的效果相混淆。一种有用的做法（在训练一批固定的数据时）是执行足够多次迭代，这样早停法便不会起作用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
